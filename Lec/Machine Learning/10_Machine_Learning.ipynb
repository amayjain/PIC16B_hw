{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Machine learning is the science (and art) of programming computers so they can learn from data. \n",
    "\n",
    "For example:\n",
    "- Ranking your web search result (Google)\n",
    "- Recommendation system: Amazon\n",
    "- AlphaGo\n",
    "- Chatgpt or Large Language Model (Large Language Model)\n",
    "- Spam filter\n",
    "- Image classification\n",
    "- Detecting credit card fraud\n",
    "- ...\n",
    "\n",
    "In general, we hope to use previous data to design(train) some algorithms(model), then use trained models to predict when we receive new data.\n",
    "\n",
    "- Example 1: We can use emails (data) to construct a spam filter (algorithm, model), then we can use spam filter to predict that new emails (new data) is a spam or not.\n",
    "\n",
    "- Example 2: We can use your current friends list (data) to construct a friend recommendation system (algorithm, model), then we can use this model to find people (new data) who you may know.\n",
    "\n",
    "- ...\n",
    "\n",
    "So, our goal is to finding a model that can be used on **unseen data**.\n",
    "\n",
    "\n",
    "## Machine Learning pipeline:\n",
    "\n",
    "1. Look at the Big Picture:\n",
    "    \n",
    "    - You should understand your task, e.g regression (predict housing price) or classification (handwritten numbers).\n",
    "    - Select a performance measure\n",
    "    - ...\n",
    "\n",
    "2. Get the Data\n",
    "\n",
    "    - Download the data from internet\n",
    "    - ...\n",
    "\n",
    "3. Discover your data and visualize your data\n",
    "\n",
    "    - Pandas and Matplotlib\n",
    "    - ...\n",
    "\n",
    "4. Prepare your data for Machine Learning Algorithm\n",
    "\n",
    "    - Algorithm only accepts numeric values\n",
    "    - Data cleaning (fillna, dropna)\n",
    "    - Text and categorical attributes to numerics\n",
    "    - ...\n",
    "\n",
    "5. Select and train your model\n",
    "\n",
    "\n",
    "6. Fine-tune your model\n",
    "\n",
    "    - Cross-validation\n",
    "    - try different models and ensemble them\n",
    "    - evaluate your model on test set\n",
    "\n",
    "7. Launch, monitor, and maintain your model\n",
    "\n",
    "\n",
    "\n",
    "# Supervised learning\n",
    "\n",
    "Supervised learning means that you can use both input $x$ and output $y$ to train a model. The output $y$ is also called *labels*.\n",
    "\n",
    "If the labels are \"continuous\" real numbers (e.g. stock price, house price, insurance fee, height, weights, and etc ), then it is **regression problem**.\n",
    "\n",
    "If the labels are discrete categories (e.g. dog vs cat, True vs False, handwritten digits numbers(0-9), 0 vs 1, or -1 vs 1, and etc), then it is **classification problem**.\n",
    "\n",
    "\n",
    "We will also discuss unsupervised learning and semisupervised learning problem.\n",
    "\n",
    "\n",
    "## Why do we do train/test split?\n",
    "\n",
    "- Remember that our goal is to create a model such that it can be generalized to unseen data set.\n",
    "\n",
    "- It is easy to design some trivial models that performs well on training data set but fail to work on new data points. This phenomenon is called **overfitting**.\n",
    "\n",
    "\n",
    "## No Free Luch (NFL) theorem:\n",
    "\n",
    "A model is a simplified version of the observations. The simplifications are meants to discard the superfluous details that are unlikely to generalize to new instances. To decide what data to discard and what data to keep, you must make *assumptions*. For example, a linear model makes the assumption that the data is fundamentally linear and that the distance between instances and the straight line is just noise, which can safely be ignored.\n",
    "\n",
    "In a famous 1996 paper, see [here](https://direct.mit.edu/neco/article/8/7/1341/6016/The-Lack-of-A-Priori-Distinctions-Between-Learning), David Wolpert demonstrated that if you make no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best model is linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better. The only way to know for sure which model is to evaluate them all. \n",
    "\n",
    "Since this is not possible, in practice you make some reasonable assumptions (based on visualization or data preprocessing) and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression example\n",
    "\n",
    "In this part, we make a fake regression dataset and then train a linear regression model. The main goal is to review the basic machine learning process:\n",
    "\n",
    "Generating fake dataset:\n",
    "\n",
    "    sklearn.datasets.make_regression(n_samples=100, n_features=100, *, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100) (20000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, Y = make_regression(n_samples=20000, n_features = 100, noise=1)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we need to normalize the training data, like what we did in PCA. I skip this step here because I am using randomly generated data and each column are from the same distribution or in the same scale. \n",
    "\n",
    "After normalization, we can train the model, evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 1.01203\n",
      "MSE is 1.01203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# fit model \n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# evaluate on the test data\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Since it is a regression problem, mean squared error is a common metric to evaluate the performance\n",
    "# you can use built-in command to compute the error\n",
    "mse1 = mean_squared_error(y_test, y_pred)\n",
    "print(f'MSE is {mse1:.5f}')\n",
    "\n",
    "# or you can use your own code\n",
    "mse2 = np.linalg.norm(y_pred-y_test)**2/np.size(y_test)\n",
    "print(f'MSE is {mse2:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "Notice that there are ten informative features, let's try to select them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "MSE is 1.00215\n",
      "MSE is 1.00215\n"
     ]
    }
   ],
   "source": [
    "# count the number of informative features\n",
    "print( np.sum(reg.coef_>=0.1) )\n",
    "\n",
    "# select the index\n",
    "idx = np.argwhere(reg.coef_>=0.1).squeeze()\n",
    "\n",
    "# define a new training and test dataset\n",
    "X_train_reduced = X_train[:,idx]\n",
    "X_test_reduced = X_test[:,idx]\n",
    "\n",
    "# fit model \n",
    "reg1 = LinearRegression().fit(X_train_reduced, y_train)\n",
    "\n",
    "# evaluate on the test data\n",
    "y_pred = reg1.predict(X_test_reduced)\n",
    "\n",
    "# compute error\n",
    "mse1 = mean_squared_error(y_test, y_pred)\n",
    "print(f'MSE is {mse1:.5f}')\n",
    "\n",
    "# or you can use your own code\n",
    "mse2 = np.linalg.norm(y_pred-y_test)**2/np.size(y_test)\n",
    "print(f'MSE is {mse2:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification example\n",
    "\n",
    "We can use the `make_classification` command to generate synthetic data samples.\n",
    "\n",
    "    sklearn.datasets.make_classification(n_samples=100, n_features=20, *, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, Y = make_classification(n_samples=10000, n_features = 30, n_informative=6, n_classes=4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: K-nearest neighbors\n",
    "\n",
    "#### Algorithm explanation:\n",
    "\n",
    "An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "#### Regression:\n",
    "K-nearest neighbors algorithm can also be used for regression problem. In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k = 1, then the output is simply assigned to the value of that single nearest neighbor.\n",
    "\n",
    "#### Documentation:\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.74\n",
      "Test accuracy is 0.74\n",
      "[[0.625 0.125 0.125 0.125]\n",
      " [0.75  0.25  0.    0.   ]\n",
      " [0.    0.    1.    0.   ]\n",
      " ...\n",
      " [0.125 0.5   0.25  0.125]\n",
      " [0.625 0.    0.    0.375]\n",
      " [0.125 0.625 0.    0.25 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# train the model\n",
    "neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "# test accuracy\n",
    "y_pred = neigh.predict(X_test)\n",
    "acc = neigh.score(X_test, y_test)\n",
    "print(f'Test accuracy is {acc:.2f}')\n",
    "\n",
    "# compute test accuracy without using score\n",
    "acc1 = np.sum(y_pred.reshape(y_test.shape) == y_test) / np.size(y_test)\n",
    "print(f'Test accuracy is {acc1:.2f}')\n",
    "\n",
    "# return probability\n",
    "prob = neigh.predict_proba(X_test)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use cross-validation to select the best number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "See wikipedia for model explanation: https://en.wikipedia.org/wiki/Logistic_regression\n",
    "\n",
    "**Logistic regression is designed for classification problems only. In other words, you can not use this model for regression problems.**\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.59\n",
      "Test accuracy is 0.59\n",
      "[[0.56281303 0.04135957 0.07570028 0.32012711]\n",
      " [0.20904043 0.51212522 0.15466231 0.12417203]\n",
      " [0.67553186 0.12558172 0.16790161 0.03098482]\n",
      " ...\n",
      " [0.22122758 0.59547218 0.11085617 0.07244407]\n",
      " [0.79096746 0.02564928 0.00510108 0.17828218]\n",
      " [0.07873112 0.75625907 0.1178367  0.04717311]]\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Log = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# test accuracy\n",
    "y_pred = Log.predict(X_test)\n",
    "acc = Log.score(X_test, y_test)\n",
    "print(f'Test accuracy is {acc:.2f}')\n",
    "\n",
    "# compute test accuracy without using score\n",
    "acc1 = np.sum(y_pred.reshape(y_test.shape) == y_test) / np.size(y_test)\n",
    "print(f'Test accuracy is {acc1:.2f}')\n",
    "\n",
    "# return probability\n",
    "prob = Log.predict_proba(X_test)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree based model\n",
    "\n",
    "A decision tree looks like this: ![](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_002.png)\n",
    "\n",
    "\n",
    "A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.76\n",
      "Test accuracy is 0.76\n",
      "[[0.65454545 0.         0.23636364 0.10909091]\n",
      " [0.75       0.         0.         0.25      ]\n",
      " [0.         0.         1.         0.        ]\n",
      " ...\n",
      " [0.03688525 0.91803279 0.03278689 0.01229508]\n",
      " [0.06976744 0.06976744 0.         0.86046512]\n",
      " [0.03688525 0.91803279 0.03278689 0.01229508]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "Tree = DecisionTreeClassifier(max_depth=10).fit(X_train, y_train)\n",
    "\n",
    "# test accuracy\n",
    "y_pred = Tree.predict(X_test)\n",
    "acc = Tree.score(X_test, y_test)\n",
    "print(f'Test accuracy is {acc:.2f}')\n",
    "\n",
    "# compute test accuracy without using score\n",
    "acc1 = np.sum(y_pred.reshape(y_test.shape) == y_test) / np.size(y_test)\n",
    "print(f'Test accuracy is {acc1:.2f}')\n",
    "\n",
    "# return probability\n",
    "prob = Tree.predict_proba(X_test)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "To better understand random forest model, we should know **Ensemble Learning** first, which is an important technique in Machine Learning.\n",
    "\n",
    "#### Motivation:\n",
    "\n",
    "Suppose that you have a complex question of thousands of random people, then aggregate their answers. In many cases you will find yjay yhis aggregated answer is better than an expert's answer. This is called the *wisdom of the crowd*. Similarly, if you aggregate the predictions of a groups of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble; thus this technique is called Ensemble Learning.\n",
    "\n",
    "#### When do we use ensemble model?\n",
    "\n",
    "Usually, ensemble model works better than single model, but there is no guarantee. In my opinion, since we must try different single models due to No Free Luch Theorem, it does not hurt to ensemble all single models you have tried, and look at the performance on test dataset.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Let's ensemble KNNeighbor, Logistic Regression, and Tree model together.\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier 0.7235\n",
      "LogisticRegression 0.588\n",
      "DecisionTreeClassifier 0.71\n",
      "VotingClassifier 0.741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# define classifiers you want to use\n",
    "kn_clf = KNeighborsClassifier()\n",
    "log_clf = LogisticRegression()\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "# define max vote classifier\n",
    "voting_clf = VotingClassifier( estimators=[('kn',kn_clf),('lr',log_clf),('tree',tree_clf)],\n",
    "                             voting='hard')\n",
    "# train max vote classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# let's look at each classifier's accuracy on the test set:\n",
    "for clf in (kn_clf, log_clf, tree_clf, voting_clf):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(clf.__class__.__name__, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "In short, random forest is an ensemble of decision trees. \n",
    "\n",
    "A group of Decision Tree classifiers are trained on a different random subset of the training set. Then, you use max vote (ensemble technique) to obtain the prediction. \n",
    "\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.8245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# train random feature model\n",
    "rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# accuracy\n",
    "print( f'Test accuracy is {rf.score(X_test, y_test)}' )\n",
    "\n",
    "# return probability\n",
    "prob = rf.predict_proba(X_test)\n",
    "print(prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
