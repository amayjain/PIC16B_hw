{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Machine learning is the science (and art) of programming computers so they can learn from data. \n",
    "\n",
    "For example:\n",
    "- Ranking your web search result (Google)\n",
    "- Recommendation system: Amazon\n",
    "- AlphaGo\n",
    "- Chatgpt or Large Language Model (Large Language Model)\n",
    "- Spam filter\n",
    "- Image classification\n",
    "- Detecting credit card fraud\n",
    "- ...\n",
    "\n",
    "In general, we hope to use previous data to design(train) some algorithms(model), then use trained models to predict when we receive new data.\n",
    "\n",
    "- Example 1: We can use emails (data) to construct a spam filter (algorithm, model), then we can use spam filter to predict that new emails (new data) is a spam or not.\n",
    "\n",
    "- Example 2: We can use your current friends list (data) to construct a friend recommendation system (algorithm, model), then we can use this model to find people (new data) who you may know.\n",
    "\n",
    "- ...\n",
    "\n",
    "So, our goal is to finding a model that can be used on **unseen data**.\n",
    "\n",
    "\n",
    "## Machine Learning pipeline:\n",
    "\n",
    "1. Look at the Big Picture:\n",
    "    \n",
    "    - You should understand your task, e.g regression (predict housing price) or classification (handwritten numbers).\n",
    "    - Select a performance measure\n",
    "    - ...\n",
    "\n",
    "2. Get the Data\n",
    "\n",
    "    - Download the data from internet\n",
    "    - ...\n",
    "\n",
    "3. Discover your data and visualize your data\n",
    "\n",
    "    - Pandas and Matplotlib\n",
    "    - ...\n",
    "\n",
    "4. Prepare your data for Machine Learning Algorithm\n",
    "\n",
    "    - Algorithm only accepts numeric values\n",
    "    - Data cleaning (fillna, dropna)\n",
    "    - Text and categorical attributes to numerics\n",
    "    - ...\n",
    "\n",
    "5. Select and train your model\n",
    "\n",
    "\n",
    "6. Fine-tune your model\n",
    "\n",
    "    - Cross-validation\n",
    "    - try different models and ensemble them\n",
    "    - evaluate your model on test set\n",
    "\n",
    "7. Launch, monitor, and maintain your model\n",
    "\n",
    "\n",
    "\n",
    "# Supervised learning\n",
    "\n",
    "Supervised learning means that you can use both input $x$ and output $y$ to train a model. The output $y$ is also called *labels*.\n",
    "\n",
    "If the labels are \"continuous\" real numbers (e.g. stock price, house price, insurance fee, height, weights, and etc ), then it is **regression problem**.\n",
    "\n",
    "If the labels are discrete categories (e.g. dog vs cat, True vs False, handwritten digits numbers(0-9), 0 vs 1, or -1 vs 1, and etc), then it is **classification problem**.\n",
    "\n",
    "\n",
    "We will also discuss unsupervised learning and semisupervised learning problem.\n",
    "\n",
    "\n",
    "## Why do we do train/test split?\n",
    "\n",
    "- Remember that our goal is to create a model such that it can be generalized to unseen data set.\n",
    "\n",
    "- It is easy to design some trivial models that performs well on training data set but fail to work on new data points. This phenomenon is called **overfitting**.\n",
    "\n",
    "\n",
    "## No Free Luch (NFL) theorem:\n",
    "\n",
    "A model is a simplified version of the observations. The simplifications are meants to discard the superfluous details that are unlikely to generalize to new instances. To decide what data to discard and what data to keep, you must make *assumptions*. For example, a linear model makes the assumption that the data is fundamentally linear and that the distance between instances and the straight line is just noise, which can safely be ignored.\n",
    "\n",
    "In a famous 1996 paper, see [here](https://direct.mit.edu/neco/article/8/7/1341/6016/The-Lack-of-A-Priori-Distinctions-Between-Learning), David Wolpert demonstrated that if you make no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best model is linear model, while for other datasets it is a neural network. There is no model that is a guarantee to work better. The only way to know for sure which model is to evaluate them all. \n",
    "\n",
    "Since this is not possible, in practice you make some reasonable assumptions (based on visualization or data preprocessing) and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression example\n",
    "\n",
    "In this part, we make a fake regression dataset and then train a linear regression model. The main goal is to review the basic machine learning process:\n",
    "\n",
    "Generating fake dataset:\n",
    "\n",
    "    sklearn.datasets.make_regression(n_samples=100, n_features=100, *, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100) (20000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 20000, n_features = 100, n_informative = 10, noise = 1)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we need to normalize the training data, like what we did in PCA. I skip this step here because I am using randomly generated data and each column are from the same distribution or in the same scale. \n",
    "\n",
    "After normalization, we can train the model, evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 0.9994\n",
      "MSE is 0.9994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# fit model\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# evaluate on the test data\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# test error: y_pred and y_test\n",
    "\n",
    "mse1 = np.linalg.norm(y_pred - y_test) ** 2 / np.size(y_test)\n",
    "\n",
    "mse2 = mean_squared_error(y_pred, y_test)\n",
    "\n",
    "print(f'MSE is {mse1:.4f}')\n",
    "print(f'MSE is {mse2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "Notice that there are ten informative features, let's try to select them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[46 49 53 57 58 66 73 78 90 93]\n",
      "MSE is 0.9912\n"
     ]
    }
   ],
   "source": [
    "# count the number of informative features\n",
    "\n",
    "print(np.sum(reg.coef_ >= 0.1))\n",
    "\n",
    "# select the index\n",
    "\n",
    "idx = np.argwhere(reg.coef_ >= 0.1).squeeze()\n",
    "\n",
    "print(idx)\n",
    "\n",
    "# redefine the new training and test data\n",
    "\n",
    "X_train_reduced = X_train[:, idx]\n",
    "\n",
    "X_test_reduced = X_test[:, idx]\n",
    "\n",
    "# fit model\n",
    "\n",
    "reg1 = LinearRegression().fit(X_train_reduced, y_train)\n",
    "\n",
    "# evaluate on the test dataset\n",
    "\n",
    "y_pred1 = reg1.predict(X_test_reduced)\n",
    "\n",
    "# test error: y_pred and y_test\n",
    "\n",
    "mse = mean_squared_error(y_pred1, y_test)\n",
    "\n",
    "print(f'MSE is {mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification example\n",
    "\n",
    "We can use the `make_classification` command to generate synthetic data samples.\n",
    "\n",
    "    sklearn.datasets.make_classification(n_samples=100, n_features=20, *, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples = 20000, n_features = 30, n_informative = 6, n_classes = 4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: K-nearest neighbors\n",
    "\n",
    "Model explanation is given during the lecture.\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use cross-validation to select the best number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree based model\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "To better understand random forest model, we should know **Ensemble Learning** first, which is an important technique in Machine Learning.\n",
    "\n",
    "#### Motivation:\n",
    "\n",
    "Suppose that you have a complex question of thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert's answer. This is called the *wisdom of the crowd*. Similarly, if you aggregate the predictions of a groups of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble; thus this technique is called Ensemble Learning.\n",
    "\n",
    "Let's ensemble KNNeighbor, Logistic Regression, and Tree model together.\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "In short, random forest is an ensemble of decision tree. \n",
    "\n",
    "A group of Decision Tree classifiers are trained on a different random subset of the training set. Then, you use max vote (ensemble technique) to obtain the prediction. \n",
    "\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
