{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet: Neural Network\n",
    "\n",
    "In this worksheet, I want to answer some practical questions we have seen when we train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "\n",
    "- **Relu activation function** $\\mathop{ReLU}(x) = \\max(x,0)$\n",
    "\n",
    "\n",
    "- **Sigmoid function** $\\mathop{Sigmoid}(x) = 1 / (1 + \\exp(-x))$\n",
    "\n",
    "\n",
    "- **Leaky ReLU** $\\mathop{LeakyReLU}_\\alpha(x) = \\max(\\alpha x, x)$, usually $\\alpha=0.01$ but recently people find that $\\alpha=0.2$ is better.\n",
    "\n",
    "\n",
    "- **ELU** $\\mathop{ELU}_\\alpha(x) = \\begin{cases} \\alpha(\\exp(x)-1) \\mbox{ if } x<0 \\\\ x \\mbox{ if } x>0 \\end{cases} $\n",
    "\n",
    "- **SELU** $\\mathop{SELU}_\\alpha(x) = \\begin{cases} \\mathrm{scale}*\\alpha(\\exp(x)-1) \\mbox{ if } x<0 \\\\ \\mathrm{scale}*x \\mbox{ if } x>0 \\end{cases} $\n",
    "\n",
    "Remark: ReLU stands for rectified linear unit and (S)ELU satnds for (scaled) exponential linear unit.\n",
    "\n",
    "Which activation function should you use? In general, SELU > ELU > leaky ReLU > ReLU > sigmoid. However, if speed is your priority, ReLU might still be the bset choice since many libraries and hardware accelerators provide ReLU-specified optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers:\n",
    "\n",
    "#### Adam\n",
    "\n",
    "Official paper: https://arxiv.org/abs/1412.6980\n",
    "\n",
    "Syntax: \n",
    "\n",
    "    tf.keras.optimizers.Adam(\n",
    "        \n",
    "        learning_rate=0.001, \n",
    "        beta_1=0.9, \n",
    "        beta_2=0.999, \n",
    "        epsilon=1e-07, \n",
    "        amsgrad=False,\n",
    "        name='Adam', \n",
    "        **kwargs)\n",
    "        \n",
    "Parameters:\n",
    "\n",
    "learning_rate: rate at which algorithm updates the parameter.  \n",
    "               Tensor or float type of value.Default value is 0.001\n",
    "\n",
    "beta_1: Exponential decay rate for 1st moment. Constant Float \n",
    "        tensor or float type of value. Default value is 0.9\n",
    "        \n",
    "beta_2: Exponential decay rate for 2nd moment. Constant Float \n",
    "        tensor or float type of value. Default value is 0.999\n",
    "        \n",
    "epsilon: Small value used to sustain numerical stability. \n",
    "         Floating point type of value. Default value is 1e-07\n",
    "         \n",
    "amsgrad: Whether to use AMSGrad variant or not. \n",
    "         Default value is False.\n",
    "         \n",
    "name: Optional name for the operation\n",
    "\n",
    "**kwargs: Keyworded variable length argument length\n",
    "\n",
    "#### RMSprop\n",
    "Syntax: \n",
    "\n",
    "    tf.keras.optimizers.RMSprop(\n",
    "    \n",
    "        learning_rate=0.001, \n",
    "        rho=0.9, \n",
    "        momentum=0.0, \n",
    "        epsilon=1e-07, \n",
    "        centered=False,\n",
    "        name='RMSprop', \n",
    "        **kwargs)\n",
    "\n",
    "Parameters:\n",
    "learning_rate: rate at which algorithm updates the parameter. \n",
    "               Tensor or float type of value.Default value is 0.001\n",
    "               \n",
    "rho: Discounting factor for gradients. Default value is 0.9\n",
    "\n",
    "momentum: accelerates rmsprop in appropriate direction. \n",
    "          Float type of value. Default value is 0.0\n",
    "          \n",
    "epsilon: Small value used to sustain numerical stability. \n",
    "         Floating point type of value. Default value is 1e-07\n",
    "         \n",
    "centered: By this gradients are normalised by the variance of \n",
    "          gradient. Boolean type of value. Setting value to True may\n",
    "          help with training model however it is computationally \n",
    "          more expensive. Default value if False.\n",
    "          \n",
    "name: Optional name for the operation\n",
    "\n",
    "**kwargs: Keyworded variable length argument length.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Adagrad\n",
    "Syntax: \n",
    "\n",
    "    tf.keras.optimizers.Adagrad(\n",
    "    \n",
    "        learning_rate=0.001,\n",
    "        initial_accumulator_value=0.1,\n",
    "        epsilon=1e-07,\n",
    "        name=\"Adagrad\",\n",
    "        **kwargs)\n",
    "        \n",
    "Parameters: \n",
    "\n",
    "learning_rate: rate at which algorithm updates the parameter. \n",
    "               Tensor or float type of value.Default value is 0.001\n",
    "               \n",
    "initial_accumulator_value: Starting value for the per parameter \n",
    "                           momentum. Floating point type of value.\n",
    "                           Must be non-negative.Default value is 0.1\n",
    "                           \n",
    "epsilon: Small value used to sustain numerical stability. \n",
    "         Floating point type of value. Default value is 1e-07.\n",
    "         \n",
    "name: Optional name for the operation\n",
    "\n",
    "**kwargs: Keyworded variable length argument length\n",
    "\n",
    "#### SGD and its variations\n",
    "Syntax: \n",
    "\n",
    "    tf.keras.optimizers.SGD(\n",
    "        \n",
    "        learning_rate = 0.01,\n",
    "        momentum=0.0, \n",
    "        nesterov=False, \n",
    "        name='SGD', \n",
    "        **kwargs)\n",
    "        \n",
    "Parameters: \n",
    "\n",
    "learning_rate: rate at which algorithm updates the parameter. \n",
    "               Tensor or float type of value.Default value is 0.01\n",
    "               \n",
    "momentum: accelerates gradient descent in appropriate\n",
    "          direction. Float type of value. Default value is 0.0\n",
    "          \n",
    "nesterov: Whether or not to apply Nesterov Momentum.\n",
    "          Boolean type of value. Default value is False.\n",
    "          \n",
    "name: Optional name for the operation\n",
    "\n",
    "**kwargs: Keyworded variable length argument length.\n",
    "\n",
    "\n",
    "#### Some notes on different optimizers:\n",
    "1. https://www.geeksforgeeks.org/adam-optimizer/\n",
    "2. https://medium.com/aiÂ³-theory-practice-business/optimization-in-deep-learning-5a5d263172e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which optimizer should you use?\n",
    "\n",
    "- Convergence speed:\n",
    "\n",
    "SGD < SGD(momentum) = SGD(momentum, nesterov=TRUE) < Adagrad = RMSprop = Adam\n",
    "\n",
    "- Convergence quality:\n",
    "\n",
    "SGD = SGD(momentum) = SGD(momentum, nesterov=TRUE) >= RMSprop = Adam > Adagrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is your turn to try Tensorflow.\n",
    "\n",
    "Please run the following cell to generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 14:44:02.987477: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# generate data points\n",
    "datadim = 5              # intrinsic dimension of data\n",
    "dim = 500                # dimension of the ambient space\n",
    "N = 5000                 # data set size\n",
    "eps = 0.25               # noise level in the observaed data\n",
    "\n",
    "# create low dimensional data points\n",
    "Xreal = np.random.randn(N, datadim)\n",
    "y = np.tanh(Xreal[:,0]) + np.cos(Xreal[:,1]) - np.exp(-Xreal[:,4])\n",
    "\n",
    "# a matrix to embed the given data into a high-dimensional ambient space\n",
    "transform = np.random.randn(datadim,dim)\n",
    "\n",
    "# the high-dimensional data is obtained by matrix multiplication\n",
    "X = Xreal @ transform\n",
    "\n",
    "# making the observations noisy\n",
    "X += np.random.normal(0, eps, size = X.shape)\n",
    "y += np.random.normal(0, eps, size = y.shape)\n",
    "\n",
    "# Test data points:\n",
    "Ntest = 500\n",
    "Xrealtest = np.random.randn(Ntest, datadim)\n",
    "ytest = np.tanh(Xrealtest[:,0]) + np.cos(Xrealtest[:,1]) - np.exp(-Xrealtest[:,4])\n",
    "Xtest = Xrealtest@transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural network structure: Shallow neural network with 200 neurons in the hidden layer. Activation function is ReLU. \n",
    "\n",
    "- Loss: This is regression problem so you should use MSE as loss and metric.\n",
    "\n",
    "- Optimizers: you should try Adam optimizers, SGD optimizers, Adagrad optimizers. You probably need to change default parameters to make your optimizers working properly, especially SGD optimizers.\n",
    "\n",
    "- Number of epochs = 10, batch_size = 16\n",
    "\n",
    "You should report test mse for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 14:44:06.073103: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 825us/step - loss: 2.7244\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 794us/step - loss: 1.4858\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 752us/step - loss: 1.2638\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 0s 751us/step - loss: 0.8825\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 767us/step - loss: 0.8453\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 0s 777us/step - loss: 0.7813\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 801us/step - loss: 0.7241\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 774us/step - loss: 0.6959\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 772us/step - loss: 0.6916\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 763us/step - loss: 0.6741\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s 695us/step - loss: 1.6245\n",
      "The test mse is 1.6245051622390747\n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    \n",
    "    [\n",
    "\n",
    "        # first hidden layer\n",
    "        layers.Dense(200, input_shape = (500, ), activation = \"relu\", kernel_initializer = \"he_normal\"), # 200 neurons, 500 features\n",
    "    \n",
    "        # output\n",
    "        layers.Dense(1) # predict 1 number (regression)\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "optim_fn = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07, amsgrad = False, name = 'Adam')\n",
    "\n",
    "model.compile(optimizer = optim_fn, loss = loss_fn)\n",
    "\n",
    "history = model.fit(X, y, epochs = 10, batch_size = 16)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(f\"The test mse is {model.evaluate(Xtest, ytest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 0s 778us/step - loss: 1.0599\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 763us/step - loss: 0.8577\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 761us/step - loss: 0.7909\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 0s 760us/step - loss: 0.7647\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 797us/step - loss: 0.7481\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 0s 770us/step - loss: 0.7195\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 771us/step - loss: 0.7118\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 772us/step - loss: 0.7014\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 759us/step - loss: 0.6899\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 755us/step - loss: 0.6894\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s 697us/step - loss: 1.4805\n",
      "The test mse is 1.4805461168289185\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    \n",
    "    [\n",
    "\n",
    "        # first hidden layer\n",
    "        layers.Dense(200, input_shape = (500, ), activation = \"relu\", kernel_initializer = \"he_normal\"), # 200 neurons, 500 features\n",
    "    \n",
    "        # output\n",
    "        layers.Dense(1) # predict 1 number (regression)\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "optim_fn = tf.keras.optimizers.Adagrad(learning_rate = 0.001, initial_accumulator_value = 0.1, epsilon = 1e-07, name = \"Adagrad\")\n",
    "\n",
    "model.compile(optimizer = optim_fn, loss = loss_fn)\n",
    "\n",
    "history = model.fit(X, y, epochs = 10, batch_size = 16)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(f\"The test mse is {model.evaluate(Xtest, ytest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 0s 733us/step - loss: 1.3701\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 0s 723us/step - loss: 0.9681\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 0s 713us/step - loss: 0.8596\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 0s 709us/step - loss: 0.8205\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 0s 732us/step - loss: 0.7749\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 0s 746us/step - loss: 0.7418\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 0s 724us/step - loss: 0.7195\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 0s 714us/step - loss: 0.6826\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 0s 721us/step - loss: 0.6877\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 0s 720us/step - loss: 0.6670\n",
      "\n",
      "\n",
      "16/16 [==============================] - 0s 698us/step - loss: 1.5489\n",
      "The test mse is 1.548887014389038\n"
     ]
    }
   ],
   "source": [
    "# Model 3\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    \n",
    "    [\n",
    "\n",
    "        # first hidden layer\n",
    "        layers.Dense(200, input_shape = (500, ), activation = \"relu\", kernel_initializer = \"he_normal\"), # 200 neurons, 500 features\n",
    "    \n",
    "        # output\n",
    "        layers.Dense(1) # predict 1 number (regression)\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "optim_fn = tf.keras.optimizers.SGD(learning_rate = 0.001, momentum = 0.0, nesterov = False, name = 'SGD')\n",
    "\n",
    "model.compile(optimizer = optim_fn, loss = loss_fn)\n",
    "\n",
    "history = model.fit(X, y, epochs = 10, batch_size = 16)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(f\"The test mse is {model.evaluate(Xtest, ytest)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
