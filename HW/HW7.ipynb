{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Fake News Classification\n",
    "\n",
    "Due: 11:59pm, June 2.\n",
    "\n",
    "## Please restart the kernel and run all before you submit!\n",
    "\n",
    "## Your Name: Amay Jain\n",
    "\n",
    "Throughout the HW, function docstrings, incline comments, and markdown are required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring data\n",
    "\n",
    "Read the csv file from the given link \n",
    "\n",
    "    train_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 12:48:39.139619: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Regular expression operations\n",
    "import re  \n",
    "# built in string module: change uppercase and lowercase\n",
    "import string\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow import keras\n",
    "\n",
    "# requires update to tensorflow 2.4\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22449, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n",
    "df = pd.read_csv(train_url)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17366</td>\n",
       "      <td>Merkel: Strong result for Austria's FPO 'big c...</td>\n",
       "      <td>German Chancellor Angela Merkel said on Monday...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5634</td>\n",
       "      <td>Trump says Pence will lead voter fraud panel</td>\n",
       "      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17487</td>\n",
       "      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>\n",
       "      <td>On December 5, 2017, Circa s Sara Carter warne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12217</td>\n",
       "      <td>Thyssenkrupp has offered help to Argentina ove...</td>\n",
       "      <td>Germany s Thyssenkrupp, has offered assistance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5535</td>\n",
       "      <td>Trump say appeals court decision on travel ban...</td>\n",
       "      <td>President Donald Trump on Thursday called the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0       17366  Merkel: Strong result for Austria's FPO 'big c...   \n",
       "1        5634       Trump says Pence will lead voter fraud panel   \n",
       "2       17487  JUST IN: SUSPECTED LEAKER and “Close Confidant...   \n",
       "3       12217  Thyssenkrupp has offered help to Argentina ove...   \n",
       "4        5535  Trump say appeals court decision on travel ban...   \n",
       "\n",
       "                                                text  fake  \n",
       "0  German Chancellor Angela Merkel said on Monday...     0  \n",
       "1  WEST PALM BEACH, Fla.President Donald Trump sa...     0  \n",
       "2  On December 5, 2017, Circa s Sara Carter warne...     1  \n",
       "3  Germany s Thyssenkrupp, has offered assistance...     0  \n",
       "4  President Donald Trump on Thursday called the ...     0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Datasets\n",
    "\n",
    "Write a function called `make_dataset`. This function should do four things:\n",
    "\n",
    "1. Change the text to lowercase.\n",
    "\n",
    "2. Remove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may find this StackOverFlow thread to be helpful.\n",
    "\n",
    "3. Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column. You may find it helpful to consult lecture notes or this tutorial for reference on how to construct and use Datasets with multiple inputs.\n",
    "\n",
    "Helpful resources:\n",
    "1. Lower case: https://saturncloud.io/blog/how-to-lowercase-a-pandas-dataframe-string-column-if-it-has-missing-values/\n",
    "\n",
    "2. Remove stopwords: https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe. You need to install nltk library. If you do not want to install it, you can use Google colab.\n",
    "\n",
    "3. Tensorflow dataset with multiple inputs and set batch: https://stackoverflow.com/questions/52582275/tf-data-with-multiple-inputs-outputs-in-keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 12:49:04.715684: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(df):\n",
    "\n",
    "    data = df.copy()\n",
    "\n",
    "    data['text'] = data['text'].str.lower()\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    data['title'] = data['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    data = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "                'title': data[['title']],\n",
    "                'text': data[['text']]\n",
    "            },\n",
    "            {\n",
    "                'fake': data[['fake']]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "tf_df = make_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split:\n",
    "\n",
    "Write a function `train_test_split` to do train test split for any tensorflow dataset. The passing argument should be the training size.\n",
    "\n",
    "\n",
    "Then, you should use your function to do train_test split and the training size is 80% of your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_size = 0.8):\n",
    "\n",
    "    train_size = int(train_size * len(df))\n",
    "\n",
    "    train, test = data.take(train_size), data.skip(train_size)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "\n",
    "train, test = train_test_split(tf_df, 0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create validation data - 20% of the train data\n",
    "\n",
    "val_size = int(0.2 * len(train)) # Number of validation datapoints\n",
    "\n",
    "train_size = len(train) - val_size # New number of training datapoints after taking out validation data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Redefine new training data and create validation data\n",
    "\n",
    "train, val = train.take(train_size), train.skip(train_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Batch data to make training model faster\n",
    "\n",
    "batch_size = 50\n",
    "train, val, test = train.batch(batch_size), val.batch(batch_size), test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization\n",
    "\n",
    "Here is one option:\n",
    "\n",
    "    #preparing a text vectorization layer for tf model\n",
    "    size_vocabulary = 2000\n",
    "\n",
    "    def standardization(input_data):\n",
    "        lowercase = tf.strings.lower(input_data)\n",
    "        no_punctuation = tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation),'')\n",
    "        return no_punctuation \n",
    "\n",
    "    title_vectorize_layer = TextVectorization(\n",
    "        standardize=standardization,\n",
    "        max_tokens=size_vocabulary, # only consider this many words\n",
    "        output_mode='int',\n",
    "        output_sequence_length=500) \n",
    "\n",
    "    title_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n",
    "    \n",
    "You can also use your preferred vectorization, e.g. text vectorization. You are also welcome to change the parameters such as `size_vocabulary` and `output_sequence_length`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing a text vectorization layer for tf model\n",
    "size_vocabulary = 2000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def standardization(input_data):\n",
    "    \n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    \n",
    "    no_punctuation = tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation),'')\n",
    "    \n",
    "    return no_punctuation \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "title_vectorize_layer = TextVectorization(\n",
    "    standardize = standardization,\n",
    "    max_tokens = size_vocabulary, # only consider this many words\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = 500) \n",
    "\n",
    "text_vectorize_layer = TextVectorization(\n",
    "    standardize = standardization,\n",
    "    max_tokens = size_vocabulary, # only consider this many words\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = 500) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "title_vectorize_layer.adapt(train.map(lambda x, y: x['title']))\n",
    "\n",
    "text_vectorize_layer.adapt(train.map(lambda x, y: x['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models\n",
    "\n",
    "Please use Keras models to offer a perspective on the following question:\n",
    "\n",
    "`When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?`\n",
    "To address this question, create three (3) Keras models.\n",
    "\n",
    "In the first model, you should use only the article title as an input.\n",
    "\n",
    "In the second model, you should use only the article text as an input.\n",
    "\n",
    "In the third model, you should use both the article title and the article text as input.\n",
    "\n",
    "Train your models on the training data until they appear to be “fully” trained. Assess and compare their performance. Make sure to include a visualization of the training histories.\n",
    "\n",
    "\n",
    "## Notes:\n",
    "\n",
    "1. For the first two models, you don’t have to create new Datasets. Instead, just specify the inputs to the keras.Model appropriately, and Keras will automatically ignore the unused inputs in the Dataset.\n",
    "\n",
    "2. The lecture notes and tutorials linked above are likely to be helpful as you are creating your models as well.\n",
    "\n",
    "3. You will need to use the Functional API, rather than the Sequential API, for this modeling task.\n",
    "\n",
    "4. When using the Functional API, it is possible to use the same layer in multiple parts of your model; see this [tutorial](https://keras.io/guides/functional_api/) for examples. I recommended that you share a text vectorization layer and an embedding layer for both the article title and text inputs.\n",
    "    \n",
    "    Note: Do not use the shared embedding layer with separate text vectorization layers. If you do so, you will be embedding two different words on the same coordinate.\n",
    "\n",
    "5. You may encounter overfitting, in which case Dropout layers can help.\n",
    "\n",
    "You’re free to be creative when designing your models. If you’re feeling very stuck, start with some of the pipelines for processing text that we’ve seen in lecture, and iterate from there. Please include in your discussion some of the things that you tried and how you determined the models you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_input = keras.Input(\n",
    "    shape = (1,), \n",
    "    name = \"title\",\n",
    "    dtype = \"string\"\n",
    ")\n",
    "\n",
    "text_input = keras.Input(\n",
    "    shape = (1,), \n",
    "    name = \"text\",\n",
    "    dtype = \"string\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_features = title_vectorize_layer(title_input)\n",
    "title_features = layers.Embedding(size_vocabulary, 3, name = \"embedding_title\")(title_features)\n",
    "title_features = layers.Dropout(0.2)(title_features)\n",
    "title_features = layers.GlobalAveragePooling1D()(title_features)\n",
    "title_features = layers.Dropout(0.2)(title_features)\n",
    "title_features = layers.Dense(32, activation = 'relu')(title_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = text_vectorize_layer(text_input)\n",
    "text_features = layers.Embedding(size_vocabulary, 3, name = \"embedding_text\")(text_features)\n",
    "text_features = layers.Dropout(0.2)(text_features)\n",
    "text_features = layers.GlobalAveragePooling1D()(text_features)\n",
    "text_features = layers.Dropout(0.2)(text_features)\n",
    "text_features = layers.Dense(32, activation = 'relu')(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output for model using only the title as a feature\n",
    "output_title = layers.Dense(2, name = \"fake\")(title_features)\n",
    "\n",
    "# output for model using only the text as a feature\n",
    "output_text = layers.Dense(2, name = \"fake\")(text_features)\n",
    "\n",
    "# output for model using both the title and text as features\n",
    "# first have to concatenate both features\n",
    "main_both = layers.concatenate([title_features, text_features], axis = 1)\n",
    "main_both = layers.Dense(32, activation = 'relu')(main_both)\n",
    "output_both = layers.Dense(2, name = \"fake\")(main_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_title = keras.Model(\n",
    "    inputs = [title_input],\n",
    "    outputs = output_title\n",
    ")\n",
    "\n",
    "model_text = keras.Model(\n",
    "    inputs = [text_input],\n",
    "    outputs = output_text\n",
    ")\n",
    "\n",
    "model_both = keras.Model(\n",
    "    inputs = [title_input, text_input],\n",
    "    outputs = output_both\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_title.compile(optimizer = \"adam\",\n",
    "              loss = losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model_text.compile(optimizer = \"adam\",\n",
    "              loss = losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model_both.compile(optimizer = \"adam\",\n",
    "              loss = losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/keras/engine/functional.py:637: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    }
   ],
   "source": [
    "history_title = model_title.fit(train, \n",
    "                    validation_data = val,\n",
    "                    epochs = 20, \n",
    "                    verbose = False)\n",
    "\n",
    "history_text = model_text.fit(train, \n",
    "                    validation_data = val,\n",
    "                    epochs = 20, \n",
    "                    verbose = False)\n",
    "\n",
    "history_both = model_both.fit(train, \n",
    "                    validation_data = val,\n",
    "                    epochs = 20, \n",
    "                    verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_title.history[\"accuracy\"], label=\"training1\")\n",
    "plt.plot(history_text.history[\"accuracy\"], label=\"training2\")\n",
    "plt.plot(history_both.history[\"accuracy\"], label=\"training3\")\n",
    "\n",
    "plt.plot(history_title.history[\"val_accuracy\"], label=\"vtraining1\")\n",
    "plt.plot(history_text.history[\"val_accuracy\"], label=\"vtraining2\")\n",
    "plt.plot(history_both.history[\"val_accuracy\"], label=\"vtraining3\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharey = False, figsize = (20, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Accuracy Should You Aim For?\n",
    "\n",
    "Your three different models might have noticeably different performance. Your best model should be able to consistently score at least 97% validation accuracy.\n",
    "\n",
    "After comparing the performance of each model on validation data, make a recommendation regarding the question at the beginning of this section. Should algorithms use the title, the text, or both when seeking to detect fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now we’ll test your model performance on unseen test data. For this part, you can focus on your best model, and ignore the other two.\n",
    "\n",
    "Once you’re satisfied with your best model’s performance on validation data, download the test data here:\n",
    "\n",
    "    test_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading items:\n",
    "\n",
    "\n",
    "#### Data Prep\n",
    "1. Stopwords are removed during the construction of the data set.\n",
    "\n",
    "2. make_dataset is implemented as a function, and used to create both the training/validation and testing data sets.\n",
    "\n",
    "3. The constructed Dataset has multiple inputs.\n",
    "\n",
    "4. Write a function to do train test split. \n",
    "\n",
    "5. 20% of the training data is split off for validation.\n",
    "\n",
    "#### Models\n",
    "6. Model 1 uses only the article title.\n",
    "\n",
    "7. Model 2 uses only the article text.\n",
    "\n",
    "8. Model 3 uses both the article title and text.\n",
    "\n",
    "9. For model 3, embedding is consistent with the text vectorization method. i.e., if you use shared embedding layer, the preceding text vectorization layer also should be shared.\n",
    "\n",
    "10. The training history is plotted for each of the three models, including the training and validation performance.\n",
    "\n",
    "11. The most performant model is evaluated on the test data set.\n",
    "\n",
    "12. The best model consistently obtains at least 97% accuracy on the validation set.\n",
    "\n",
    "13. The best model’s performance on the test set is shown.\n",
    "\n",
    "#### Style and Documentation\n",
    "\n",
    "14. Throughout the HW, function docstrings, incline comments, and markdown are required. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
