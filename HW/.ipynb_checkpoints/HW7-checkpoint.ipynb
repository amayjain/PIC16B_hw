{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Fake News Classification\n",
    "\n",
    "Due: 11:59pm, June 2.\n",
    "\n",
    "## Please restart the kernel and run all before you submit!\n",
    "\n",
    "## Your Name: \n",
    "\n",
    "Throughout the HW, function docstrings, incline comments, and markdown are required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring data\n",
    "\n",
    "Read the csv file from the given link \n",
    "\n",
    "    train_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Datasets\n",
    "\n",
    "Write a function called `make_dataset`. This function should do four things:\n",
    "\n",
    "1. Change the text to lowercase.\n",
    "\n",
    "2. Remove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may find this StackOverFlow thread to be helpful.\n",
    "\n",
    "3. Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column. You may find it helpful to consult lecture notes or this tutorial for reference on how to construct and use Datasets with multiple inputs.\n",
    "\n",
    "Helpful resources:\n",
    "1. Lower case: https://saturncloud.io/blog/how-to-lowercase-a-pandas-dataframe-string-column-if-it-has-missing-values/\n",
    "\n",
    "2. Remove stopwords: https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe. You need to install nltk library. If you do not want to install it, you can use Google colab.\n",
    "\n",
    "3. Tensorflow dataset with multiple inputs and set batch: https://stackoverflow.com/questions/52582275/tf-data-with-multiple-inputs-outputs-in-keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split:\n",
    "\n",
    "Write a function `train_test_split` to do train test split for any tensorflow dataset. The passing arguement should be the training size.\n",
    "\n",
    "\n",
    "Then, you should use your function to do train_test split and the trainning size is 80% of your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization\n",
    "\n",
    "Here is one option:\n",
    "\n",
    "    #preparing a text vectorization layer for tf model\n",
    "    size_vocabulary = 2000\n",
    "\n",
    "    def standardization(input_data):\n",
    "        lowercase = tf.strings.lower(input_data)\n",
    "        no_punctuation = tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation),'')\n",
    "        return no_punctuation \n",
    "\n",
    "    title_vectorize_layer = TextVectorization(\n",
    "        standardize=standardization,\n",
    "        max_tokens=size_vocabulary, # only consider this many words\n",
    "        output_mode='int',\n",
    "        output_sequence_length=500) \n",
    "\n",
    "    title_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n",
    "    \n",
    "You can also use your preferred vectorization, e.g. text vectorization. You are also welcome to change the parameters such as `size_vocabulary` and `output_sequence_length`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models\n",
    "\n",
    "Please use Keras models to offer a perspective on the following question:\n",
    "\n",
    "`When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?`\n",
    "To address this question, create three (3) Keras models.\n",
    "\n",
    "In the first model, you should use only the article title as an input.\n",
    "\n",
    "In the second model, you should use only the article text as an input.\n",
    "\n",
    "In the third model, you should use both the article title and the article text as input.\n",
    "\n",
    "Train your models on the training data until they appear to be “fully” trained. Assess and compare their performance. Make sure to include a visualization of the training histories.\n",
    "\n",
    "\n",
    "## Notes:\n",
    "\n",
    "1. For the first two models, you don’t have to create new Datasets. Instead, just specify the inputs to the keras.Model appropriately, and Keras will automatically ignore the unused inputs in the Dataset.\n",
    "\n",
    "2. The lecture notes and tutorials linked above are likely to be helpful as you are creating your models as well.\n",
    "\n",
    "3. You will need to use the Functional API, rather than the Sequential API, for this modeling task.\n",
    "\n",
    "4. When using the Functional API, it is possible to use the same layer in multiple parts of your model; see this [tutorial](https://keras.io/guides/functional_api/) for examples. I recommended that you share a text vectorization layer and an embedding layer for both the article title and text inputs.\n",
    "    \n",
    "    Note: Do not use the shared embedding layer with separate text vectorization layers. If you do so, you will be embedding two different words on the same coordinate.\n",
    "\n",
    "5. You may encounter overfitting, in which case Dropout layers can help.\n",
    "\n",
    "You’re free to be creative when designing your models. If you’re feeling very stuck, start with some of the pipelines for processing text that we’ve seen in lecture, and iterate from there. Please include in your discussion some of the things that you tried and how you determined the models you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Accuracy Should You Aim For?\n",
    "\n",
    "Your three different models might have noticeably different performance. Your best model should be able to consistently score at least 97% validation accuracy.\n",
    "\n",
    "After comparing the performance of each model on validation data, make a recommendation regarding the question at the beginning of this section. Should algorithms use the title, the text, or both when seeking to detect fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now we’ll test your model performance on unseen test data. For this part, you can focus on your best model, and ignore the other two.\n",
    "\n",
    "Once you’re satisfied with your best model’s performance on validation data, download the test data here:\n",
    "\n",
    "    test_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading items:\n",
    "\n",
    "\n",
    "#### Data Prep\n",
    "1. Stopwords are removed during the construction of the data set.\n",
    "\n",
    "2. make_dataset is implemented as a function, and used to create both the training/validation and testing data sets.\n",
    "\n",
    "3. The constructed Dataset has multiple inputs.\n",
    "\n",
    "4. Write a function to do train test split. \n",
    "\n",
    "5. 20% of the training data is split off for validation.\n",
    "\n",
    "#### Models\n",
    "6. Model 1 uses only the article title.\n",
    "\n",
    "7. Model 2 uses only the article text.\n",
    "\n",
    "8. Model 3 uses both the article title and text.\n",
    "\n",
    "9. For model 3, embedding is consistent with the text vectorization method. i.e., if you use shared embedding layer, the preceding text vectorization layer also should be shared.\n",
    "\n",
    "10. The training history is plotted for each of the three models, including the training and validation performance.\n",
    "\n",
    "11. The most performant model is evaluated on the test data set.\n",
    "\n",
    "12. The best model consistently obtains at least 97% accuracy on the validation set.\n",
    "\n",
    "13. The best model’s performance on the test set is shown.\n",
    "\n",
    "#### Style and Documentation\n",
    "\n",
    "14. Throughout the HW, function docstrings, incline comments, and markdown are required. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
