{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3 Optimization (80pts)\n",
    "\n",
    "**Due 11:59pm, Apr 29th.**\n",
    "\n",
    "## Please restart the kernel and run all before you submit !\n",
    "\n",
    "\n",
    "## Your Name: Amay Jain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: SGD with arbitrary batch size  (50pts)\n",
    "\n",
    "**The goal of this exercise is to implement SGD with arbitrary batch size for a certain linear regression model.** \n",
    "\n",
    "Given data points $(x^i,y^i)_{i=1}^m$ where each data point $x$ has three features/attributes, i.e. $x=(x_1,x_2,x_3)$, we consider the following linear regression model\n",
    "$$ y =  a_0 + a_1x_1 + a_2x_2 + a_3x_3.$$\n",
    "\n",
    "Here, $a_0,a_1,a_2,a_3$ are coefficients. \n",
    "\n",
    "The corresponding optimization problem is \n",
    "\n",
    "$$ \\mathop{\\mathrm{Loss}}(a_0,a_1,a_2,a_3) = \\frac{1}{m}\\sum_{i=1}^{m} (a_0+a_1x_1^i+a_2x_2^i+a_3x_3^i-y^i)^2 $$\n",
    "\n",
    "\n",
    "You should do the following three things:\n",
    "\n",
    "1. Write a python function to implement SGD with arbitrary batch_size. Hint: you can write it as a python function and treat batch size as a function input.\n",
    "\n",
    "2. Select at least 4 batch sizes and fix number of iteration, then run SGD for different stepsize, report the the running time for each batch size (use `print`). What is your conclusion?\n",
    "\n",
    "3. Use the same batch sizes as part 2, then draw a plot to visualize the the loss after each iteration vs iterations. What is your conclusion?\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Your SGD algorithm should allow different batch size. \n",
    "\n",
    "2. Write python functions to do 2 and 3.\n",
    "\n",
    "3. Detailed docustring is required for each function. Add necessary inline comments and markdown to explain your code or make comments.\n",
    "\n",
    "Grading is based on the codes (15 pts), function docstrings (10 pts), comments (10 pts) and conclusions (15 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to generate data points\n",
    "m = 1000                                # number of data points\n",
    "n = 3                                   # number of features\n",
    "x = np.random.randn(m, n)               # each row of x represents a data point\n",
    "theta = np.random.randn(n, 1)           # true coefficients a_1, a_2, a_3\n",
    "y = x@theta + 3 + np.random.rand(m, 1)  # observations and true a_0 = 3\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def SGD(a, eta, batchsize):\n",
    "\n",
    "    a0, a1, a2, a3 = a[0], a[1], a[2], a[3]\n",
    "    \n",
    "    j = np.random.choice(m, batchsize)\n",
    "\n",
    "    copy_x = x[j]\n",
    "    copy_y = y[j].reshape(batchsize)\n",
    "\n",
    "    x1 = copy_x[:, 0]\n",
    "    x2 = copy_x[:, 1]\n",
    "    x3 = copy_x[:, 2]\n",
    "\n",
    "    grad_a0 = sum(2 * (a0 + a1*x1 + a2*x2 + a3*x3 - copy_y)) / batchsize\n",
    "    grad_a1 = sum(2 * (a0 + a1*x1 + a2*x2 + a3*x3 - copy_y) * x1) / batchsize\n",
    "    grad_a2 = sum(2 * (a0 + a1*x1 + a2*x2 + a3*x3 - copy_y) * x2) / batchsize\n",
    "    grad_a3 = sum(2 * (a0 + a1*x1 + a2*x2 + a3*x3 - copy_y) * x3) / batchsize\n",
    "\n",
    "    return [a0 - eta*grad_a0, \n",
    "            a1 - eta*grad_a1, \n",
    "            a2 - eta*grad_a2, \n",
    "            a3 - eta*grad_a3]\n",
    "\n",
    "def running_time(batchsizes, stepsizes, fixed_batchsize = 50, fixed_stepsize = 0.01, iterations = 500):\n",
    "\n",
    "    print('Changing batchsizes:\\n')\n",
    "    \n",
    "    for batchsize in batchsizes:\n",
    "        \n",
    "        # initial coefficient values\n",
    "        a = [10, 10, 10, 10]\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(iterations + 1):\n",
    "            a = SGD(a, fixed_stepsize, batchsize)\n",
    "            \n",
    "        stop = time.time()\n",
    "        \n",
    "        print(f\"For batchsize = {batchsize} and stepsize = {fixed_stepsize}, the computational time is {stop - start} seconds.\\n\")\n",
    "\n",
    "    print('\\n')\n",
    "        \n",
    "    print('Changing stepsizes:\\n')\n",
    "    \n",
    "    for stepsize in stepsizes:\n",
    "        \n",
    "        # initial coefficient values\n",
    "        a = [10, 10, 10, 10]\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(iterations + 1):\n",
    "            a = SGD(a, stepsize, fixed_batchsize)\n",
    "            \n",
    "        stop = time.time()\n",
    "        \n",
    "        print(f\"For batchsize = {fixed_batchsize} and stepsize = {stepsize}, the computational time is {stop - start} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(batchsizes, fixed_stepsize = 0.01, iterations = 500):\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, sharey = False, figsize = (20, 12))\n",
    "\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "    x3 = x[:, 2]\n",
    "\n",
    "    copy_y = y.reshape(m)\n",
    "\n",
    "    for j in range (4):\n",
    "\n",
    "        # initial coefficient values\n",
    "        a = [10, 10, 10, 10]\n",
    "        \n",
    "        losses = []\n",
    "\n",
    "        for i in range(iterations + 1):\n",
    "    \n",
    "            a = SGD(a, 0.01, batchsizes[j])\n",
    "            \n",
    "            L = sum((a[0] + a[1]*x1 + a[2]*x2 + a[3]*x3 - copy_y)**2) / m\n",
    "            \n",
    "            losses.append(L)\n",
    "\n",
    "        if j < 2:\n",
    "\n",
    "            ax[0, j].plot(range(iterations + 1), losses)\n",
    "\n",
    "            ax[0, j].set_title(f'Loss for batchsize = {batchsizes[j]}')\n",
    "\n",
    "        else:\n",
    "\n",
    "            ax[1, j - 2].plot(range(iterations + 1), losses)\n",
    "\n",
    "            ax[1, j - 2].set_title(f'Loss for batchsize = {batchsizes[j]}')\n",
    "\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsizes = [100, 200, 400, 800]\n",
    "\n",
    "stepsizes = [0.05, 0.1, 0.2, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_time(batchsizes = batchsizes, stepsizes = stepsizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(batchsizes = batchsizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your explanation on the relation between batch size and number of iterations. (please change this cell to markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: SGD animation for polynomial regression (30 pts)\n",
    "\n",
    "In this exercise, our goal is to create a similar animation (which is available [here](https://nbviewer.org/github/PhilChodrow/PIC16B/blob/master/lectures/math/optimization.ipynb)) for polynomial regression model.  \n",
    "\n",
    "\n",
    "#### Comments:\n",
    "You cannot copy the code in the link directly because \n",
    "\n",
    "1. the sample code is for simple linear regression only, but we are considering polynomial regression.\n",
    "\n",
    "2. the data generating process is included in the class initialization, however training data are provided in this exercise.\n",
    "\n",
    "Please understand the sample code first, and then modify it accordingly. It is also fine if you do not write a class (class is used in the sample code) to do the animation.\n",
    "\n",
    "#### Grading policy:\n",
    "1. There is no error in your code.\n",
    "2. The animation result looks reasonable.\n",
    "3. Docstring and comments are required for your code.\n",
    "\n",
    "\n",
    "#### Mathematics on polynomial regression:\n",
    "Given data points $(x_i,y_i)_{i=1}^m$, polynomial regression is considered if we believe that the true relation between input $x$ and output $y$ can be described by a polynomial $y=a_0+a_1x+\\cdots+a_nx^n$ for some degree $n$.\n",
    "\n",
    "The goal is to find coefficients $a_0,\\cdots,a_n$ by minimizing the loss function\n",
    "$$ \\mathop{\\mathrm{minimize}}_{a_o,\\cdots,a_n} \\frac{1}{m}\\sum_{i=1}^m (a_0+a_1x_i+\\cdots+a_nx_i^n-y_i)^2 $$\n",
    "\n",
    "Then we can use SGD to solve this optimization problem.\n",
    "\n",
    "#### Data Generation\n",
    "\n",
    "Please run the following code to generate the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of observations\n",
    "m = 100\n",
    "# generate input x \n",
    "X = np.random.uniform(0, 3, m)\n",
    "X.sort()\n",
    "# generate output y\n",
    "y = -2 - 2*X + X**2 + 0.1*np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdatePlot:\n",
    "    \"\"\"\n",
    "    an update class for a polynomial regression stochastic gradient descent (SGD)\n",
    "    animation. Takes instance variables corresponding to plot objects, \n",
    "    parameters and estimates, and animation controls. Calling the class\n",
    "    updates the plot. \n",
    "    \n",
    "    ax: array of 2 axes on which to plot\n",
    "    a0, a1, a2: floats, true parameters of polynomial regression\n",
    "    a0_, a1_, a2_: floats, initial estimates of the true parameters\n",
    "    eps: float, learning rate\n",
    "    m: int, number of data points\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ax, a0, a1, a2, a0_, a1_, a2_, m, X, y, eps = 0.01):\n",
    "        \n",
    "        # store the axes on which to plot\n",
    "        self.ax0 = ax[0]\n",
    "        self.ax1 = ax[1]\n",
    "        \n",
    "        # true coefficients\n",
    "        self.a0 = a0\n",
    "        self.a1 = a1\n",
    "        self.a2 = a2\n",
    "        \n",
    "        # initial coefficient estimates\n",
    "        self.a0_ = a0_\n",
    "        self.a1_ = a1_\n",
    "        self.a2_ = a2_\n",
    "        \n",
    "        # number of data points\n",
    "        self.m = m\n",
    "        \n",
    "        # create data\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # used for plotting lines\n",
    "        self.x_space = np.linspace(0, 3, 100)\n",
    "        \n",
    "        # show data on lefthand plot\n",
    "        self.ax0.scatter(self.X, self.y, color = \"grey\", s = 4, zorder = 100)\n",
    "        \n",
    "        # prepare animation variables\n",
    "        self.point = self.ax0.scatter([], [], color = \"red\", zorder = 200)\n",
    "        self.line, = self.ax0.plot([], [], 'k-')\n",
    "        self.loss, = self.ax1.plot([], [], 'k-')\n",
    "        \n",
    "        # learning rate\n",
    "        self.eps = eps\n",
    "        \n",
    "        # timesteps and value of loss function\n",
    "        self.t = []\n",
    "        self.L = []\n",
    "        \n",
    "    def __call__(self, i):\n",
    "        \"\"\"\n",
    "        makes the class callable, resulting in plot updates\n",
    "        each call performs a single step of SGD and returns\n",
    "        appropriate artists. i represents the algorithm \n",
    "        timestep/animation frame. \n",
    "        \"\"\"\n",
    "        \n",
    "        # pick a random point\n",
    "        j = np.random.randint(self.m)      \n",
    "        x, y = self.X[j], self.y[j]\n",
    "        \n",
    "        # update estimates with gradient of loss function evaluated \n",
    "        # at that point with respect to the coefficients\n",
    "        # \"this is the math\"\n",
    "        self.a0_ -= self.eps*2*(self.a0_ + self.a1_*x + self.a2_*(x)**2 - y)\n",
    "        self.a1_ -= self.eps*2*(self.a0_ + self.a1_*x + self.a2_*(x)**2 - y)*x\n",
    "        self.a2_ -= self.eps*2*(self.a0_ + self.a1_*x + self.a2_*(x)**2 - y)*(x**2)\n",
    "        \n",
    "        # evaluate loss function \n",
    "        # \"this is the other math\"\n",
    "        L = sum((self.a0_ + self.a1_*self.X + self.a2_*(self.X)**2 - self.y)**2) / self.m\n",
    "        \n",
    "        # update lefthand plot with highlight point and new regression line\n",
    "        self.point.set_offsets([[x, y]])\n",
    "        self.line.set_data(self.x_space, self.a0_ + self.a1_*self.x_space + self.a2_*(self.x_space)**2)\n",
    "        \n",
    "        # update timestep and loss function\n",
    "        self.t.append(i)\n",
    "        self.L.append(L)\n",
    "        \n",
    "        # plot loss function\n",
    "        self.loss.set_data(self.t, self.L)\n",
    "        \n",
    "        # return artists\n",
    "        return [self.point, self.line, self.loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of algorithm iterations\n",
    "n_steps = 500\n",
    "\n",
    "# create plotting background\n",
    "fig, ax = plt.subplots(1, 2, figsize = (7, 3))\n",
    "\n",
    "ax[0].set_xlim(-1, 4)\n",
    "ax[0].set_ylim(-4, 2)\n",
    "\n",
    "ax[1].set_xlim(0, n_steps)\n",
    "ax[1].set_ylim(0, 10)\n",
    "ax[1].grid(True)\n",
    "\n",
    "ax[0].set(title = \"Regression Problem\", \n",
    "          xlabel = r\"$x$\",\n",
    "          ylabel = r\"$y$\")\n",
    "\n",
    "ax[1].set(title = \"Current Loss\",\n",
    "          xlabel = \"Iteration\",\n",
    "          ylabel = r\"$\\mathcal{L}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# create the plot updater\n",
    "# assuming initial coefficients are all 0\n",
    "ud = UpdatePlot(ax, -2, -2, 1, 0, 0, 0, m, X, y)\n",
    "anim = FuncAnimation(fig, ud, frames = n_steps, interval = 200, blit = True)\n",
    "\n",
    "# to interactively display in notebook\n",
    "plt.close() # prevents plot from showing twice\n",
    "HTML(anim.to_jshtml()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
